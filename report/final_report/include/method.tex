%% kolla att allt är i imperfekt
%% verb i passiv form

\section{Choose of tool for verification}
Software unit testing can be achieved by almost any tool.
%% motivera.
Consequently this phase is not the most interesting when it comes to the choose
of a tool verification. Of course one can take the simplicity to achieve good
%% "simplicity" - av vad?
unit testing into account, but still it is not what makes a verification tool
especially unique for the project goals.

Since the purpose is about benchmarking software the phase ''verification of
software safety requirements'' will not influence the choose. To be able to test
this phase, a greater amount of components of the whole system must be
available. Such components may include hardware etcetera. Implementation wise
%% "etcetera" borde arbetas bort.
should everything be able to run on a standard PC-machine.

The most interesting part is the software integration and testing. Is there a
tool that one can use to easily combine test and requirements from different
modules? Is it possible to test functional safety concept from this
combination, for example by corrupting some software elements?

\subsection{Why QuickCheck and Erlang?}

\section{Specification}
In AUTOSAR, specifications for each module is given in text form. Consequently
one must first, before a module can be tested, implement the specification for
that module in code.

\section{Testing}
%% vilka properties? quickcheck?
%% kanske är bättre att skriva "Module properties have to take..."
%% alternativt "Quickcheck properties for a module have to take..."
Properties for a module have to take the current state in consideration, since
most functions written in an imperative language are not immutable. This gives
raise to the idea of a state based testing tool.
%% .. och plötsligt: en lista:...
\begin{itemize}
\item Choose a specification which will be translated to QuickCheck properties
in parts.
\item With the use of statistics and confidence intervals, show that, with
enough tests the state-space will be exhausted.
\item Evaluate other semi formal techniques and show that the results from them
shows that QuickCheck is reliable for verification.
\item Generalize the technique.
\end{itemize}


\section{Choice of AUTOAR module to test}
There were several modules that were up for discussion when it came to the
choice of a module to test. Since the goal was to get a proof of concept for
that it was possible to
get an ASIL-classification and achieve functional safety using Quickcheck, it
seemed preferable to choose a less complicated module. It was also desirable to
have the actual C code and not just library files.

\section{Implementation}
The C-code that was to be tested, using Quickcheck, was already unit tested and
run sharp in lab environments at Mecel.

The behavior of the WdgM module was completely implemented in Erlang independent of
the design choose in the C-code. The idea was to not get inspired by methods used
in the C-code and possibly, if the C-code contained errors, end up with the same
faults.

The implementation of the module was done to be able to test API calls,
as described in theory, against the C-code and check that postconditions
held, according to figure \ref{FIG:api_calls}. The postconditions were written
to test that AUTOSAR requirements held. In other words that the API calls were
called correctly. A problem when writing the postconditions, because AUTOSAR
specification are written in natural language, was to translate words into code.
It was easy to see that there were room for different interpretations, which most
likely would result in errors later.

The implementation of the AUTOSAR module in Erlang was done
in an iterative way. Every piece of code were not required to be implemented before
tests could be run. This because of that a module in AUTOSAR consist of several API
calls. Hence it was enough to implement the specification for one API call
before tests could be run. Of course this gave rise to that only a part of
the C-code was tested. Also early tests may not have fully tested the API
call because there may have been branches in the C-code, for this call, that
were never reached unless other API calls were called.
\begin{figure}[!h]
\label{FIG:api_calls}
\begin{center}
\includegraphics{pictures/api_calls.jpg}
\end{center}
\caption{Shows Erlang modeled states with calls against the C-code}
\end{figure}
Early in the implementation face Quickcheck found differences between the Erlang
and C-implementation. This was expected because every programmer makes mistakes.
The question regarding who was faulty arose? The C-code or the Erlang code? Then
the API was thoroughly read and a conclusion was made. Either a bug in the
C-code was found or the Erlang-code needed to be corrected. There were however
cases when the API was ambiguous. In those cases the C-interpretation was chosen
as correct and the ambiguous specification written down.

Surprisingly bugs in the C-code was found early, even though it already run
sharp in lab environments; only a few API calls needed to be implemented in Erlang.

If a bug in the C-code was discovered how should the work be continued? Since
Quickcheck terminates as soon as it finds a counter example something was needed to
be able to run further tests. Some alternatives were discussed.

\begin{enumerate}
        \item Fix the C-code, in other words change the C source code.
                \label{ENUMERATE:FixCCode}
        \item Mocking, in other words simulate different C-code output.
        \item Change the Erlang module to a faulty behavior to follow the C
                implementation.
\end{enumerate}

Item \ref{ENUMERATE:FixCCode} was chosen, see section \ref{sec:handlebugs}.

When thoroughly reading the AUTOSAR API not only ambiguous rules were found also
rules that contradicted each other were recognized. In those cases the
implementation in the C-code was followed.

A great tool when a clear interpretation of the AUTOSAR specification was not
possible was to look into the C-code. Even though Quickcheck can be used to
test libraries when the actually source code is not available.

When the full module was implemented in Erlang code there had to be some assurance
for that every piece of code in the C implementation was actually tested.
Code coverage for the Erlang implementation was measured using the erlang module
\emph{cover}. To be able to to measure the code coverage of the C-code the
commercial tool Bullseye Coverage was used. Only by using those tools it was
easy to see that the result was not good enough. The main problem seemed to be
that the WdgM was put in an absorbing state and after that following commands
did not test anything interesting. The reason for that an absorbing state was
reached was the availing of negative testing. The testing was negative because
invalid command sequences and arguments were generated.

The next step was to tweak the generators, used by Quickcheck, to construct
valid API-calls. There were several branches in the C-code
that needed a specific sequence of API calls, with correct arguments, to be
reached. Also the tweaking of the generators could be implemented in a iterative
way. Change the probability properties of the generators and check the results.
The results were analyzed and the generators were tweaked even more to make the
results even better.

Just checking code coverage was however not good enough for our goals ...

To get a better picture of the work flow used in this thesis see
figure \ref{fig:workflow}.

\begin{figure}[!ht]
\caption{Work flow}
\label{fig:workflow}
\fbox{
\parbox{\linewidth}{
\begin{enumerate}
\item Construct a model for an AUTOSAR module in Erlang
\item Run Quickcheck for this model and compare the results with the output from
the c code. \label{compare}
\item Tweak the generators for the test cases \label{generators}
    \item Evaluate the results
    \begin{enumerate}
      \item What is the state space?
      \item Which test cases are relevant?
      \item Collapse irrelevant states
      \item What can be said about the results?
    \end{enumerate}
\item Are the results good enough, does it satisfy the requirements for the ASIL
levels?
\item If not go the step \ref{compare}
\end{enumerate}
}}
\end{figure}
%even though correction of the C-code
%was not part of the thesis. Since there were actually code available, not
%only libraries, and most bugs was easy to fix this seemed like the best choice,
%see results.

A challenging step is the analysis of the results. If the testing tool returns zero
errors what does that say about the robustness of the input byte code? Passed
100 of 100 tests is just a statement and does not say anything more than that
some tests passed. Can tests be implemented in a clever way so that it is
possible to get some kind of confidence interval on the correctness of the code?

\section{Configurations}
When the code coverage was calculated it was recognized that every piece of code
wasn't executed. The reason seemed to be that the current configuration
disallowed, even though the program behaved correctly, the execution of those
lines of code. Because of that the implementation of the Erlang module was done
independent of configuration it was easy to to run tests on several
configuration which resulted in that everything was executed.

The tweaking of generators, to achieve better test cases, seemed in some sense to be
configuration dependent. Better test cases were generated if the generators were
tweaked according to a specific configuration, see results.

\begin{figure}[!ht]
\input{pictures/box.tex}
\caption{Abstract implementation module}
\end{figure}

\section{Calling the API commands}
\label{SEC:CALLING_COMMANDS}
API calls were executed by QuickCheck using the \emph{run\_commands/1} function
according to \ref{SEC:QuickCheckIntro}. The runtime environment module (RTE) is
however responsible for the scheduling of the main function, which should be
executed in a given time interval. Since the RTE was not available when testing
the watchdog manager the main function was called randomly and it was
assumed that every time the main function was called a given amount of time had
passed.

Except the main function only one internal algorithm used by the watchdog
manager was time dependent, namely deadline supervision. A dead line supervision
consist of two checkpoints. One start checkpoint, one stop checkpoint and a
maximum time it shall take to reach the stop checkpoint after the start
checkpoint was reached. The AUTOSAR specification was however lacking of a
clear definition of how time should be represented. The C code just used ticks,
not actual time stamp, which was incremented every time the main function was
called. It was in other words assumed that the RTE was able to execute the main
function correctly and a fixed amount of ticks would always represent the same
amount of time. Accepting this solution it was easy to adopt the same approach
in the Erlang module. More about this can be found in section
\ref{SEC:FUNCTIONAL_SAFETY_TIME}.

